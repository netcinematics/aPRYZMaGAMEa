# AI META_NET

Artificial Intelligence Meta Network.

- provides actual context mapping for common phrases.

- self-generative mapping of concepts not yet mapped.

- actual plastistic meta awareness study & game.

---

## PURPOSE

The LIBZ folder contains any random source of MARKDOWN, HTML, or TXT files, as a search source.
The SCRIPTZ folder contains JS scripts to run that search LIBZ and produce output in CARDZ and TXTZ
The CARDZ folder contains JSON objects of meta_data for card views.
The TXTZ folder contains JSON objects of text_data for txt views.

These files are directly served from raw github pages. A node server was previously available for this type of routing on the backend, but given significant grief to deploy on vercel, and fragility of networks over time, it was realized that raw self-service in (singular) gh-pages dependency would be most likely to last longest in production.

---

## TASKS

X - meta_net/ taxonomy data files.
X - meta_net/scriptz/       SCRIPTZ
X - meta_net/token_cardz/   CARDZ
X - meta_net/token_markdown/ LIBZ
X - meta_net/token_txtz       TXTZ
X - meta_net/readme.md
X - meta_net/meta_net_schema.js
X - clean up libz files...
X - import scriptz
X - organize scriptz

---

# ALGORITHMS

X first-class-function architecture for standard_tokenizer to advanced_tokenizer
X add version tag to output
X add timestamp tag to output
X add key
X add txtz only - no txt, txt become type.
X replace ~ tilde wrap with single underscore _aWordza
X end of quote is a tilde.

X line_end replace by ;
X script numerator_1 _2 naming
X date folder DATA/tokenz_YMD_2023_7_26

X continue for every token
X output each array as build step
X endline rollup by watcher index and fn.
X mint_token_factory()
X add reduced /TOKEN_TEST/ file.md
X add TOPICZ and SUBSETZ
X MAINTOPIC and SUBTOPIC s
X remove empty section top/bottom


X finish QUOTE: write_all_quotes. end_line.
X finish SIG, DATE, write_all.
X finish SERIEZ: numz,
X NUMZ on seriez, txt, and quotez
X item drop off check.
X add NUMZ
X ENDZ_KEY
X MIXCASE_KEY
X UNIVERSAL_KEY
X PRIME_KEY
X total_token_count
X 4 star to triple_exclaim !!! key_conzept
X inline YMD search, and push.
X All key tokenz avoid SIG_ and YMD_

X finish TXTZ separate repopulation as KEY_CARDZ. 
X all txtz that contain key. cross-reference-idx.

X write txt_tgt to card.txtz, push numz
X search for PRIME_KEYS for card header. origin



X key aliases across key_types.
X cardz lookup, key_conzeptz, 
X confirm cardz hold all_quotez
X write all txt hitz to cardz
X remove duplicate keys


X txt objects in txtz cardz, why? keyz as linkz on txt
O keyz and alias on {type:txt}
O each token scans itself for keys and alias.

O test across all test files
O remove test mode
O open up multiple files.
O write to file for all keys (read all keys)
O show txt_tgt below with meta.linkz

O random colors for keys

O convert topics into card_awordza_2.json
X reduce omni to prime keys only
O create cards for prime keys
O use alias attribute to define aliases (statically)
O write to cards with alias lookup.
O txt format per cards.

O add in special_operator functionality
O add in alias_: line-check, functionality.
O push to aliaz_: array.
O push to numz array.
O fix  create_token_cardz
O first prime item empty.

O remove dupe aseeka in prime_key_idx
O remove ** and () in topics

O ~0~ THE_END



EMOJIS: https://www.w3schools.com/charsets/ref_emoji.asp

